
import math
import torch
import sympy as sym
import numpy as np
from scipy.optimize import brentq
from scipy import special as sp
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_min


def get_bond(name):
    if name == 'qm9':
        bond = torch.tensor([
            [[0.74, 1.09, 1.01, 0.96, 0.92],
             [1.09, 1.54, 1.47, 1.43, 1.35],
             [1.01, 1.47, 1.45, 1.40, 1.36],
             [0.96, 1.43, 1.40, 1.48, 1.42],
             [0.92, 1.35, 1.36, 1.42, 1.42]],
            [[0, 0, 0, 0, 0],
             [0, 1.34, 1.29, 1.20, 0],
             [0, 1.29, 1.25, 1.21, 0],
             [0, 1.20, 1.21, 1.21, 0],
             [0, 0, 0, 0, 0]],
            [[0, 0, 0, 0, 0],
             [0, 1.20, 1.16, 1.13, 0],
             [0, 1.16, 1.10, 0, 0],
             [0, 1.13, 0, 0, 0],
             [0, 0, 0, 0, 0]]
        ])
        bond_mask = torch.tensor([
            [[1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1]],
            [[0, 0, 0, 0, 0],
             [0, 1, 1, 1, 0],
             [0, 1, 1, 1, 0],
             [0, 1, 1, 1, 0],
             [0, 0, 0, 0, 0]],
            [[0, 0, 0, 0, 0],
             [0, 1, 1, 1, 0],
             [0, 1, 1, 0, 0],
             [0, 1, 0, 0, 0],
             [0, 0, 0, 0, 0]]
        ], dtype=torch.long)
    elif name == 'drugs':
        bond = torch.tensor([
            [[0.74, 1.19, 1.09, 1.01, 0.96, 0.92, 0.00, 1.48, 1.44, 1.34, 1.27, 1.52, 1.41, 1.61, 0.00, 0.00],
             [1.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00],
             [1.09, 0.00, 1.54, 1.47, 1.43, 1.35, 0.00, 1.85, 1.84, 1.82, 1.77, 0.00, 1.94, 2.14, 0.00, 0.00],
             [1.01, 0.00, 1.47, 1.45, 1.40, 1.36, 0.00, 0.00, 1.77, 1.68, 1.75, 0.00, 2.14, 2.22, 0.00, 0.00],
             [0.96, 0.00, 1.43, 1.40, 1.48, 1.42, 0.00, 1.63, 1.63, 1.51, 1.64, 0.00, 1.72, 1.94, 0.00, 0.00],
             [0.92, 0.00, 1.35, 1.36, 1.42, 1.42, 0.00, 1.60, 1.56, 1.58, 1.66, 0.00, 1.78, 1.87, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [1.48, 0.00, 1.85, 0.00, 1.63, 1.60, 0.00, 2.33, 0.00, 2.00, 2.02, 0.00, 2.15, 2.43, 0.00, 0.00],
             [1.44, 0.00, 1.84, 1.77, 1.63, 1.56, 0.00, 0.00, 2.21, 2.10, 2.03, 0.00, 2.22, 0.00, 0.00, 0.00],
             [1.34, 0.00, 1.82, 1.68, 1.51, 1.58, 0.00, 2.00, 2.10, 2.04, 2.07, 0.00, 2.25, 2.34, 0.00, 0.00],
             [1.27, 1.75, 1.77, 1.75, 1.64, 1.66, 0.00, 2.02, 2.03, 2.07, 1.99, 0.00, 2.14, 0.00, 0.00, 0.00],
             [1.52, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [1.41, 0.00, 1.94, 2.14, 1.72, 1.78, 0.00, 2.15, 2.22, 2.25, 2.14, 0.00, 2.28, 0.00, 0.00, 0.00],
             [1.61, 0.00, 2.14, 2.22, 1.94, 1.87, 0.00, 2.43, 0.00, 2.34, 0.00, 0.00, 0.00, 2.66, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],
            [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.34, 1.29, 1.20, 0.00, 0.00, 0.00, 0.00, 1.60, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.29, 1.25, 1.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.20, 1.21, 1.21, 0.00, 0.00, 0.00, 1.50, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 1.50, 0.00, 0.00, 0.00, 0.00, 1.86, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.60, 0.00, 0.00, 0.00, 0.00, 0.00, 1.86, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],

            [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.20, 1.16, 1.13, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.16, 1.10, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 1.13, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
             [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]]
        ])
        bond_mask = torch.tensor([
            [[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],
             [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0],
             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0],
             [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
            [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
            [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
        ], dtype=torch.long)
    else:
        raise NotImplementedError('Unsupported dataset')
    margin = torch.tensor([0.01, 0.005, 0.003])
    return bond.permute(1, 2, 0) + margin.unsqueeze(0).unsqueeze(0), bond_mask.permute(1, 2, 0)


def read_discrete_feat(dataset_config):
    feat_dict = dataset_config['discrete_info']
    discrete_info = {}
    for key in feat_dict.keys():
        discrete_info[key + '_class'] = len(feat_dict[key])
        discrete_info[key] = torch.tensor(feat_dict[key]).unsqueeze(0).expand(len(feat_dict[key]), -1).unsqueeze(0)
    return discrete_info


def get_angle(vec1, vec2, keepdim=False):
    norm1 = vec1.norm(dim=-1).unsqueeze(-1)
    norm2 = vec2.norm(dim=-1).unsqueeze(-1)
    vec1 = vec1 / (norm1 + 1e-5)
    vec2 = vec2 / (norm2 + 1e-5)
    angle = torch.acos((vec1 * vec2).sum(-1))
    return angle


def local_geometry_calc(pos, pair_mask):
    i = pos.unsqueeze(2).unsqueeze(3)
    j = pos.unsqueeze(1).unsqueeze(3)
    k = pos.unsqueeze(1).unsqueeze(2)
    vecs_ij = i - j
    vecs_ik = i - k
    angle_i = get_angle(vecs_ij, vecs_ik)

    return angle_i * pair_mask


class RBF(nn.Module):
    def __init__(self, centers, gamma):
        super(RBF, self).__init__()
        self.centers = torch.tensor(centers, dtype=torch.float32)
        self.gamma = gamma

    def forward(self, x):
        x = x.unsqueeze(-1)
        return torch.exp(-self.gamma * torch.square(x - self.centers.to(x.device)))


class RBF_Emb(nn.Module):
    def __init__(self, emb_dim, rbf_param, add_time=False):
        super(RBF_Emb, self).__init__()
        self.center = rbf_param[0]
        self.gamma = rbf_param[1]
        self.rbf = RBF(self.center, self.gamma)
        self.linear = nn.Linear(len(self.center), emb_dim) if not add_time else nn.Linear(len(self.center)+1, emb_dim)

    def forward(self, feats, time=None):
        rbf = self.rbf(feats)
        if time is None:
            out_emb = self.linear(rbf)
        else:
            out_emb = self.linear(torch.cat([rbf, time.unsqueeze(-1)], dim=-1))
        return out_emb

def split_tensor(x: torch.Tensor, batch: torch.Tensor):
    length = torch.unique(batch, return_counts=True)[1].cpu()
    dup_count = tuple(length.numpy())
    split = x.split(dup_count, dim=0)
    return list(split), length


def atom_pos_to_pair_dist(pos):
    start = pos.unsqueeze(2)
    end = pos.unsqueeze(1)
    vecs = start - end
    radial = torch.sum(vecs ** 2, -1)
    dist = (vecs).norm(dim=-1)
    cord_diff = vecs / (dist + 1).unsqueeze(-1)
    return dist, radial, cord_diff


def create_mask(x):
    node_mask = torch.as_tensor(x[:, :, 0], dtype=torch.bool, device=x.device).long()
    zero_diag = torch.ones((node_mask.shape[-1], node_mask.shape[-1]), device=node_mask.device) - \
                torch.eye(node_mask.shape[-1], device=node_mask.device)
    pair_mask = node_mask.unsqueeze(-1) * node_mask.unsqueeze(-2) * zero_diag
    return node_mask.unsqueeze(-1), pair_mask.unsqueeze(-1)


def remove_mean_with_mask(x, node_mask):
    N = node_mask.sum(1, keepdims=True)
    mean = torch.sum(x, dim=1, keepdim=True) / N
    x = x - mean * node_mask
    return x


def sigma_and_alpha_t_given_s(gamma_t, gamma_s):
    sigma2_t_given_s = -torch.expm1(F.softplus(gamma_s) - F.softplus(gamma_t))
    # alpha_t_given_s = alpha_t / alpha_s
    log_alpha2_t = F.logsigmoid(-gamma_t)
    log_alpha2_s = F.logsigmoid(-gamma_s)
    log_alpha2_t_given_s = log_alpha2_t - log_alpha2_s

    alpha_t_given_s = torch.exp(0.5 * log_alpha2_t_given_s)
    sigma_t_given_s = torch.sqrt(sigma2_t_given_s)
    return sigma2_t_given_s, sigma_t_given_s, alpha_t_given_s


def sample_center_gravity_zero_gaussian(size, node_mask, device):
    x = torch.randn(size, device=device)
    x_masked = x * node_mask

    x = remove_mean_with_mask(x_masked, node_mask)
    return x


def sample_gaussian(size, node_mask, device):
    x = torch.randn(size, device=device)
    x = x * node_mask
    return x
